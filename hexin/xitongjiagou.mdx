---
title: "RM-01 开发者使用指南"
description: "全面了解 RM-01 的系统架构、模块配置、网络设置及模型部署方法"
icon: "code"
---

## 概述

本指南为开发者提供 RM-01 便携式超级计算机的完整技术文档，涵盖系统架构、网络配置、模型部署等核心内容：

<CardGroup cols={3}>
<Card title="网络配置" icon="network-wired" href="#网络配置">
    配置网络连接，实现设备与主机的数据交互
</Card>

<Card title="系统架构" icon="microchip" href="#系统架构">
    了解推理模组、应用模组和管理芯片的协同工作机制
</Card>

<Card title="模型部署" icon="brain" href="#模型部署">
    掌握 AI 模型的部署、配置和优化方法
</Card>
</CardGroup>

<Warning>
**使用前必读**

RM-01 由一个**推理模组**、一个**应用模组**和一颗**加密与管理芯片**（以下简称管理模组）构成，三者通过**板载以太网交换芯片**互联，形成内部局域网子网。当用户通过 **USB Type-C** 接口连接主机（如 PC、手机、iPad）时，RM-01 会通过 USB Ethernet 功能为主机虚拟出一个以太网接口，主机随即获得 IP 地址并自动加入该子网实现数据交互。
</Warning>

<Info>
设备上电启动并经 **USB Type-C** 接口连接主机后，系统将自动配置本地网络子网，**用户主机**将被分配静态 IP 地址 `10.10.99.100`。**推理模组**（IP: `10.10.99.98`）和**应用模组**（IP: `10.10.99.99`）——均部署独立的 SSH 服务，支持用户通过标准 SSH 客户端（如 OpenSSH、PuTTY）直接访问，而管理模组则需通过串口工具访问。
</Info>

## 网络配置

### 如何通过主机为 RM-01 提供互联网访问（以 macOS 为例）

在通过 USB Type-C 连接**用户主机**后，RM-01 将在网络接口列表中显示为：
- **`AX88179A`**（开发者版本）
- **`RMinte RM-01`**（商业发行版本）

<Steps>
<Step title="打开系统设置">
打开 **系统设置**（System Settings）
</Step>

<Step title="进入网络共享">
进入 **网络**（Network） → **共享**（Sharing）
</Step>

<Step title="启用互联网共享">
启用 **互联网共享**（Internet Sharing）
</Step>

<Step title="配置共享设置">
点击共享设置旁的 **"i"图标**，进入配置界面：
- 将 **"共享以下来源的连接"**（Share your connection from）设置为：**Wi-Fi**
- 在 **"使用以下端口共享给设备"**（To computers using）中，勾选：**AX88179A** 或 **RMinte RM-01**（根据设备型号选择）
</Step>

<Step title="完成配置">
点击 **完成**（Done）
</Step>

<Step title="手动配置网络接口">
返回 **网络**（Network）设置页面，手动配置 RM-01 的网络接口：
- **IP 地址**：`10.10.99.100`
- **子网掩码**：`255.255.255.0`
- **路由器**（Router）：`10.10.99.100`（即主机自身 IP）
</Step>
</Steps>

<Note>
此配置将主机作为网关，为 RM-01 提供 NAT 网络访问。RM-01 的默认网关和 DNS 均由主机通过 DHCP 服务自动分配，手动设置 IP 可确保其始终位于 `10.10.99.0/24` 子网内，与设备内部服务通信一致。
</Note>

## 系统架构

### 关于 CFexpress Type-B 存储卡

**CFexpress Type-B** 存储卡是 RM-01 设备的核心组件之一，承担系统引导、模型推理框架部署及 ISV/SV 软件分发与授权认证的关键功能。

该存储卡划分为三个独立分区：

<CardGroup cols={3}>
<Card title="rm01sys" icon="hard-drive">
**系统分区**

推理模组的操作系统与核心运行环境安装于该分区。

<Warning>
严禁用户或开发者访问、修改或删除该分区内容。任何未经授权的更改均可能导致推理模组无法启动或推理功能失效。
</Warning>
</Card>

<Card title="rm01app" icon="cube">
**应用分区**

用于暂存用户或开发者提交的 Docker 镜像文件。镜像写入后，系统将自动迁移至内置 NVMe SSD 并完成容器化部署。

<Warning>
请勿直接在该分区中运行或修改应用文件。
</Warning>
</Card>

<Card title="rm01models" icon="database">
**模型分区**

专用于存储大规模人工智能模型（如 LLM、多模态模型等）。

<Info>
关于模型格式、大小限制、加载流程及兼容性要求，请参阅下文"关于模型"章节。
</Info>
</Card>
</CardGroup>

### 关于应用模组

<Info>
**网络配置**
- IP地址: `10.10.99.99`
- 端口范围: `59000-59299`
</Info>

#### 应用模组硬件规格

```text
处理器：Intel Core i3-N305（8 核 8 线程，基础频率 1.8 GHz，最大睿频 3.8 GHz）
内存：16 GB / 24 GB LPDDR5-4800MT/s（板载，不可扩展）
存储：512 GB / 1 TB / 2 TB (可选) NVMe SSD
```

#### 应用模组 SSH 访问凭证

<CodeGroup>
```bash SSH 登录命令
ssh rm01@10.10.99.99
# 默认用户名：rm01
# 默认密码：rm01（出厂预设，仅用于首次登录）
```

```bash 修改密码
# 首次登录后立即执行
passwd
```
</CodeGroup>

<Warning>
**安全须知**

为保障系统安全，首次通过 SSH 登录后，请立即使用 `passwd` 命令修改默认密码；默认密码仅用于初始配置，严禁在生产及交付环境中使用。
</Warning>

### 关于推理模组

<Info>
**网络配置**
- IP 地址：`10.10.99.98`
- 服务端口范围：`58000–58999`
</Info>

**推理模组**是 RM-01 的核心计算单元，支持多种高性能 AI 推理配置，用户可根据模型规模与性能需求选择对应型号。

#### 硬件配置选项

| 显存   | 显存带宽   | 算力                | Tensor Core 数量 |
|--------|------------|:--------------------|:-----------------|
| 32 GB  | 204.8 GB/s | 200 TOPS  (INT8)    | 56               |
| 64 GB  | 204.8 GB/s | 275 TOPS  (INT8)    | 64               |
| 64 GB  | 273 GB/s   | 1,200 TFLOPS  (FP4) | 64               |
| 128 GB | 273 GB/s   | 2,070 TFLOPS  (FP4) | 96               |

#### 预装推理框架

RM-01 出厂时，**CFexpress Type-B** 存储卡中预装以下两个推理框架，均运行于推理模组：

<Tabs>
<Tab title="vLLM">
- **状态**: 自动启动
- **默认端口**: 58000
- **功能**: 提供 OpenAI 兼容 API 接口
- **支持请求**: 标准 POST `/v1/chat/completions` 等
</Tab>

<Tab title="TEI (Text Embedding Inference)">
- **状态**: 需手动启动
- **功能**: 文本嵌入（Embedding）服务
</Tab>
</Tabs>

#### API 访问方式

成功加载模型后，可通过以下地址访问 **vLLM** 推理服务：

```bash
http://10.10.99.98:58000/v1/chat/completions
```

<Tip>
支持标准 OpenAI 客户端（如 openai-python、curl、Postman）直接调用。
</Tip>

<Warning>
**安全须知**

为保障系统安全与稳定性，推理模组不开放 SSH 访问权限，用户与开发者无法通过任何方式直接登录或交互式操作该模块的底层操作系统。

任何试图绕过安全策略、直接访问推理模组的操作，均可能导致系统异常、数据损坏或服务中断，且不在保修服务范围内。
</Warning>

## 模型部署

### 关于模型

RM-01 支持推理多种人工智能模型，包括但不限于：

<CardGroup cols={5}>
<Card title="LLM" icon="comments">
大语言模型
</Card>
<Card title="MLM" icon="images">
多模态模型
</Card>
<Card title="VLM" icon="eye">
视觉语言模型
</Card>
<Card title="Embedding" icon="vector-square">
文本嵌入模型
</Card>
<Card title="Reranker" icon="ranking-star">
重排序模型
</Card>
</CardGroup>

<Info>
所有模型文件均需存储于设备内置的 **CFexpress Type-B** 存储卡中，用户需使用兼容的 **CFexpress Type-B** 读卡器在主机端进行模型的上传、管理与更新。
</Info>

当 **CFexpress Type-B** 存储卡接入 RM-01 后，系统会将其挂载为名为 `models` 的只读数据卷，路径为 `/home/rm01/models`。其标准文件结构如下：

```bash
models/
├── auto/                  # 系统自动加载模型目录（生产级部署）
│   ├── embedding/         # 嵌入模型（系统不自动加载，见下文说明）
│   ├── llm/               # 大语言模型（权重文件直接存放，见下文）
│   └── reranker/          # 重排序模型（系统不自动加载，见下文）
└── dev/                   # 开发者自定义模型目录（高优先级）
    ├── embedding/
    ├── llm/
    └── reranker/
```

<Note>
- `auto/` 目录用于**轻量级、标准化模型部署**，由系统自动识别
- `dev/` 目录用于**开发者精细控制模型行为**，优先级高于 `auto/`，系统将忽略 `auto/` 中的模型
</Note>

### 部署模式选择

<Tabs>
<Tab title="自动模式（auto）">

适用于快速验证和标准化部署的简化模式。

<AccordionGroup>
<Accordion icon="file-import" title="使用方式">
将模型的**完整权重文件**（如 `.safetensors`、`.bin`、`.pt`、`.awq` 等）**直接放置于 `auto/llm/` 目录下**，**禁止使用子文件夹嵌套**。

<CodeGroup>
```text ❌ 错误示例
auto/llm/Qwen3-30B-A3B-Instruct-2507-AWQ/model.safetensors
```

```text ✅ 正确示例
auto/llm/model-001-of-006.safetensors
auto/llm/config.json
auto/llm/tokenizer.json
auto/llm/vocab.json
```
</CodeGroup>
</Accordion>

<Accordion icon="gear" title="系统行为">
- 设备开机后，系统将扫描 `auto/llm/` 目录，自动加载符合兼容格式的模型
- **不支持嵌入模型（embedding）与重排序模型（reranker）的自动加载**，仅支持 LLM
- 模型加载后，**默认启用基础推理能力**，**不开启以下高级功能**：
  - Speculative Decoding（推测解码）
  - Prefix Caching（前缀缓存）
  - Chunked Prefill（分块预填充）
  - 最大上下文长度（`max_model_len`）将被限制为系统安全阈值（通常 ≤ 8192 tokens）
- **性能优化受限**：为保障系统稳定性与多任务并发能力，自动模式下的模型将使用保守的显存分配策略（`gpu_memory_utilization` ≤ 0.8）
</Accordion>
</AccordionGroup>

<Warning>
**重要提示**

自动模式适用于**快速验证模型兼容性**或**标准化部署场景**，**不适用于生产级高性能推理**。如需完整性能，请使用**手动模式（dev）**。
</Warning>

</Tab>

<Tab title="手动模式（dev）">

开发者模式，提供完整的配置控制和性能优化能力。

<Info>
**手动模式优先级高于自动模式**。当 `dev/` 的 `embedding`、`llm`、`reranker` 三个目录下存在任意 `.yaml` 配置文件时，系统将**完全忽略 `auto/` 中的所有模型**。
</Info>

#### 配置文件结构

在 `dev/` 的 `embedding`、`llm`、`reranker` 三个目录下，需分别放置以下三个 YAML 配置文件，分别用于启动对应模型服务：

| 文件名 | 用途 |
|--------|------|
| `embedding_run.yaml` | 启动嵌入模型服务（Text Embedding） |
| `llm_run.yaml` | 启动大语言模型服务（LLM） |
| `reranker_run.yaml` | 启动重排序模型服务（Reranker） |

<Note>
所有文件必须位于 `dev/` 目录下相对应的文件夹内，**文件名必须完全匹配**，大小写敏感。
</Note>

#### 示例配置文件

```yaml llm_run.yaml
model: /home/rm01/models/dev/llm/Qwen3-30B-A3B-Instruct-2507-AWQ
port: 58000
gpu_memory_utilization: 0.85
max_model_len: 24576
served_model_name: RM-01 LLM
enable_prefix_caching: true
enable_chunked_prefill: true
max_num_batched_tokens: 512
block_size: 16
tensor_parallel_size: 1
dtype: auto
```

<Tip>
**配置说明**
- `model` 路径必须为**绝对路径**，指向模型文件所在目录（非压缩包）
- `port` 必须位于 `58000–58999` 范围内，且不可与其他服务冲突
- `gpu_memory_utilization` 建议设置为 0.7–0.9，以最大化吞吐量
- 所有参数均遵循 [vLLM 官方文档规范](https://docs.vllm.ai/en/latest/index.html)
</Tip>

#### 部署步骤

<Steps>
<Step title="复制模型文件">
将模型文件（完整目录）复制至 `dev/llm/`（或 `embedding/`、`reranker/`）
</Step>

<Step title="编写配置文件">
编写并放置对应的 `.yaml` 配置文件
</Step>

<Step title="插入存储卡并重启">
插入 **CFexpress Type-B** 存储卡并重启 RM-01
</Step>

<Step title="系统自动加载">
系统将自动加载 `dev/` 中的配置，并启动对应服务
</Step>

<Step title="访问服务">
服务启动后，可通过 `http://10.10.99.98:58000/v1/chat/completions` 等接口访问
</Step>
</Steps>

<Tip>
**建议**：首次部署时，建议使用 `vLLM` 官方提供的 `--load-format auto` 和 `--dtype auto`，以自动适配模型格式。
</Tip>

</Tab>
</Tabs>

### 安全与维护须知

<Warning>
- **禁止直接 SSH 登录推理模组**：所有模型管理必须通过 `CFexpress Type-B` 存储卡完成
- **模型文件必须为原始权重**：禁止使用压缩包（.zip/.tar.gz）、加密包或非标准格式
- **文件权限**：所有模型文件需为可读（`chmod 644`），目录需可执行（`chmod 755`）
</Warning>

<Tip>
- **版本控制**：建议使用 Git 或文件命名规范（如 `Qwen3-30B-A3B-Instruct-v1.2-20250930`）管理模型版本
- **备份建议**：每次更新模型前，请备份 `dev/` 和 `auto/` 目录，避免配置丢失
</Tip>

### 模式选择建议

| 场景 | 推荐模式 | 说明 |
|------|----------|------|
| 快速验证模型是否兼容 | 自动模式（auto） | 无需配置，即插即用 |
| 生产环境高性能推理 | 手动模式（dev） + 精细配置 | 完整性能优化 |
| 多模型并行部署 | 手动模式（dev） + 多个 `.yaml` 文件 | 灵活的服务编排 |
| 开发调试、原型验证 | 手动模式（dev） | 完全控制权 |

## 技术支持

<CardGroup cols={2}>
<Card title="开发者文档" icon="book-open" href="https://docs.rminte.com">
    完整的 API 参考和技术文档
</Card>
<Card title="GitHub 仓库" icon="github" href="https://github.com/rminte">
    示例代码和开源工具
</Card>
<Card title="技术论坛" icon="comments" href="https://forum.rminte.com">
    开发者社区和技术讨论
</Card>
<Card title="技术支持" icon="headset" href="mailto:support@rminte.com">
    专业技术支持服务
</Card>
</CardGroup>

---

<div className="text-center text-sm text-gray-500 mt-8">
  © 2025 泛灵（成都）人工智能科技有限公司 版权所有
</div>